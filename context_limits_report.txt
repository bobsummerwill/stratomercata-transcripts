======================================================================
AI PROVIDER CONTEXT WINDOW TEST RESULTS
======================================================================

Test Date: 2025-11-11 12:52:03

For typical 60-90 minute transcripts:
  Estimated tokens: 20,000 - 40,000 tokens
  Plus system prompts: ~5,000 tokens
  Total needed: ~45,000 tokens maximum

======================================================================


Anthropic: claude-sonnet-4-5-20250929
  Advertised: 200,000 tokens (standard), 1,000,000 (beta)
  Maximum Tested: 50,050 tokens
  Recommended Safe Limit: 47,547 tokens (with 5% buffer)
  Chunking Needed: NO - Perfect for your transcripts!

  Test Results:
    ✓ 10,051 tokens - OK (2.3s)
    ✓ 50,050 tokens - OK (3.1s)
    ✗ 100,049 tokens - FAILED

OpenAI: gpt-4o-2025-08-06
  Advertised: 128,000 tokens
  Maximum Tested: 0 tokens
  Recommended Safe Limit: 0 tokens (with 5% buffer)
  Chunking Needed: YES - Transcripts may need splitting

  Test Results:
    ✗ 10,051 tokens - FAILED

Google: gemini-2.0-flash-exp
  Advertised: 128,000 tokens (some claim 1M)
  Maximum Tested: 120,041 tokens
  Recommended Safe Limit: 114,038 tokens (with 5% buffer)
  Chunking Needed: NO - Perfect for your transcripts!

  Test Results:
    ✓ 10,051 tokens - OK (0.7s)
    ✓ 50,050 tokens - OK (2.8s)
    ✓ 100,049 tokens - OK (1.8s)
    ✓ 120,041 tokens - OK (1.3s)
    ✗ 128,065 tokens - FAILED

DeepSeek: deepseek-chat
  Advertised: 64,000 tokens
  Maximum Tested: 64,053 tokens
  Recommended Safe Limit: 60,850 tokens (with 5% buffer)
  Chunking Needed: NO - Perfect for your transcripts!

  Test Results:
    ✓ 10,051 tokens - OK (1.7s)
    ✓ 30,017 tokens - OK (2.7s)
    ✓ 50,050 tokens - OK (4.9s)
    ✓ 64,053 tokens - OK (5.9s)

Ollama: qwen2.5:32b
  Advertised: 32,768 tokens (model dependent)
  Maximum Tested: 32,027 tokens
  Recommended Safe Limit: 30,425 tokens (with 5% buffer)
  Chunking Needed: YES - Transcripts may need splitting

  Test Results:
    ✓ 2,011 tokens - OK (8.1s)
    ✓ 4,021 tokens - OK (15.2s)
    ✓ 8,041 tokens - OK (15.5s)
    ✓ 16,014 tokens - OK (15.4s)
    ✓ 32,027 tokens - OK (15.5s)

======================================================================

RECOMMENDATIONS FOR YOUR TRANSCRIPTS:
======================================================================

BEST CHOICE: Google (gemini-2.0-flash-exp)
  Context: 120,041 tokens
  Perfect for your transcripts without chunking
  Maintains full context for best quality

ALTERNATIVES:
  - Anthropic: 50,050 tokens
  - DeepSeek: 64,053 tokens

REQUIRE CHUNKING (not ideal for quality):
  - OpenAI: 0 tokens
  - Ollama: 32,027 tokens